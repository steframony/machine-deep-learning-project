{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:10:01.652317Z",
     "iopub.status.busy": "2025-06-21T10:10:01.652063Z",
     "iopub.status.idle": "2025-06-21T10:11:50.912626Z",
     "shell.execute_reply": "2025-06-21T10:11:50.911912Z",
     "shell.execute_reply.started": "2025-06-21T10:10:01.652297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install dgl==1.1.1\n",
    "!pip install dgl==2.0.0 -f https://data.dgl.ai/wheels/cu121/repo.html\n",
    "#!pip install faiss_cpu\n",
    "!pip install faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:50.914274Z",
     "iopub.status.busy": "2025-06-21T10:11:50.914040Z",
     "iopub.status.idle": "2025-06-21T10:11:55.363310Z",
     "shell.execute_reply": "2025-06-21T10:11:55.362448Z",
     "shell.execute_reply.started": "2025-06-21T10:11:50.914253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -c \"import dgl;print(dgl.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:55.364884Z",
     "iopub.status.busy": "2025-06-21T10:11:55.364527Z",
     "iopub.status.idle": "2025-06-21T10:11:55.484856Z",
     "shell.execute_reply": "2025-06-21T10:11:55.483966Z",
     "shell.execute_reply.started": "2025-06-21T10:11:55.364844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir ./p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:55.487000Z",
     "iopub.status.busy": "2025-06-21T10:11:55.486751Z",
     "iopub.status.idle": "2025-06-21T10:11:56.461426Z",
     "shell.execute_reply": "2025-06-21T10:11:56.460348Z",
     "shell.execute_reply.started": "2025-06-21T10:11:55.486964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/p/utils\n",
    "!mkdir /kaggle/working/p/scripts\n",
    "!mkdir /kaggle/working/p/models\n",
    "!mkdir /kaggle/working/p/datasetml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:56.462842Z",
     "iopub.status.busy": "2025-06-21T10:11:56.462592Z",
     "iopub.status.idle": "2025-06-21T10:11:57.326955Z",
     "shell.execute_reply": "2025-06-21T10:11:57.326048Z",
     "shell.execute_reply.started": "2025-06-21T10:11:56.462816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/yjxiong/clustering-benchmark.git p/clustering-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARTELLA UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:57.328530Z",
     "iopub.status.busy": "2025-06-21T10:11:57.328244Z",
     "iopub.status.idle": "2025-06-21T10:11:57.530237Z",
     "shell.execute_reply": "2025-06-21T10:11:57.529497Z",
     "shell.execute_reply.started": "2025-06-21T10:11:57.328497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/misc.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextColors:\n",
    "    HEADER = \"\\033[35m\"\n",
    "    OKBLUE = \"\\033[34m\"\n",
    "    OKGREEN = \"\\033[32m\"\n",
    "    WARNING = \"\\033[33m\"\n",
    "    FATAL = \"\\033[31m\"\n",
    "    ENDC = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\"\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, name=\"task\", verbose=True):\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"[Time] {} consumes {:.4f} s\".format(\n",
    "                    self.name, time.time() - self.start\n",
    "                )\n",
    "            )\n",
    "        return exc_type is None\n",
    "\n",
    "\n",
    "def set_random_seed(seed, cuda=False):\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def l2norm(vec):\n",
    "    vec /= np.linalg.norm(vec, axis=1).reshape(-1, 1)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def is_l2norm(features, size):\n",
    "    rand_i = random.choice(range(size))\n",
    "    norm_ = np.dot(features[rand_i, :], features[rand_i, :])\n",
    "    return abs(norm_ - 1) < 1e-6\n",
    "\n",
    "\n",
    "def is_spmat_eq(a, b):\n",
    "    return (a != b).nnz == 0\n",
    "\n",
    "\n",
    "def aggregate(features, adj, times):\n",
    "    dtype = features.dtype\n",
    "    for i in range(times):\n",
    "        features = adj * features\n",
    "    return features.astype(dtype)\n",
    "\n",
    "\n",
    "def mkdir_if_no_exists(path, subdirs=[\"\"], is_folder=False):\n",
    "    if path == \"\":\n",
    "        return\n",
    "    for sd in subdirs:\n",
    "        if sd != \"\" or is_folder:\n",
    "            d = os.path.dirname(os.path.join(path, sd))\n",
    "        else:\n",
    "            d = os.path.dirname(path)\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "\n",
    "def stop_iterating(\n",
    "    current_l,\n",
    "    total_l,\n",
    "    early_stop,\n",
    "    num_edges_add_this_level,\n",
    "    num_edges_add_last_level,\n",
    "    knn_k,\n",
    "):\n",
    "    # Stopping rule 1: run all levels\n",
    "    if current_l == total_l - 1:\n",
    "        return True\n",
    "    # Stopping rule 2: no new edges\n",
    "    if num_edges_add_this_level == 0:\n",
    "        return True\n",
    "    # Stopping rule 3: early stopping, two levels start to produce similar numbers of edges\n",
    "    if (\n",
    "        early_stop\n",
    "        and float(num_edges_add_last_level) / num_edges_add_this_level\n",
    "        < knn_k - 1\n",
    "    ):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:57.531482Z",
     "iopub.status.busy": "2025-06-21T10:11:57.531202Z",
     "iopub.status.idle": "2025-06-21T10:11:58.692880Z",
     "shell.execute_reply": "2025-06-21T10:11:58.692133Z",
     "shell.execute_reply.started": "2025-06-21T10:11:57.531458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/metrics.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics.cluster import (\n",
    "    contingency_matrix,\n",
    "    normalized_mutual_info_score,\n",
    ")\n",
    "\n",
    "__all__ = [\"pairwise\", \"bcubed\", \"nmi\", \"precision\", \"recall\", \"accuracy\"]\n",
    "\n",
    "\n",
    "def _check(gt_labels, pred_labels):\n",
    "    if gt_labels.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"gt_labels must be 1D: shape is %r\" % (gt_labels.shape,)\n",
    "        )\n",
    "    if pred_labels.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"pred_labels must be 1D: shape is %r\" % (pred_labels.shape,)\n",
    "        )\n",
    "    if gt_labels.shape != pred_labels.shape:\n",
    "        raise ValueError(\n",
    "            \"gt_labels and pred_labels must have same size, got %d and %d\"\n",
    "            % (gt_labels.shape[0], pred_labels.shape[0])\n",
    "        )\n",
    "    return gt_labels, pred_labels\n",
    "\n",
    "\n",
    "def _get_lb2idxs(labels):\n",
    "    lb2idxs = {}\n",
    "    for idx, lb in enumerate(labels):\n",
    "        if lb not in lb2idxs:\n",
    "            lb2idxs[lb] = []\n",
    "        lb2idxs[lb].append(idx)\n",
    "    return lb2idxs\n",
    "\n",
    "\n",
    "def _compute_fscore(pre, rec):\n",
    "    return 2.0 * pre * rec / (pre + rec)\n",
    "\n",
    "\n",
    "def fowlkes_mallows_score(gt_labels, pred_labels, sparse=True):\n",
    "    \"\"\"The original function is from `sklearn.metrics.fowlkes_mallows_score`.\n",
    "    We output the pairwise precision, pairwise recall and F-measure,\n",
    "    instead of calculating the geometry mean of precision and recall.\n",
    "    \"\"\"\n",
    "    (n_samples,) = gt_labels.shape\n",
    "\n",
    "    c = contingency_matrix(gt_labels, pred_labels, sparse=sparse)\n",
    "    tk = np.dot(c.data, c.data) - n_samples\n",
    "    pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples\n",
    "    qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples\n",
    "\n",
    "    avg_pre = tk / pk\n",
    "    avg_rec = tk / qk\n",
    "    fscore = _compute_fscore(avg_pre, avg_rec)\n",
    "\n",
    "    return avg_pre, avg_rec, fscore\n",
    "\n",
    "\n",
    "def pairwise(gt_labels, pred_labels, sparse=True):\n",
    "    _check(gt_labels, pred_labels)\n",
    "    return fowlkes_mallows_score(gt_labels, pred_labels, sparse)\n",
    "\n",
    "\n",
    "def bcubed(gt_labels, pred_labels):\n",
    "    _check(gt_labels, pred_labels)\n",
    "\n",
    "    gt_lb2idxs = _get_lb2idxs(gt_labels)\n",
    "    pred_lb2idxs = _get_lb2idxs(pred_labels)\n",
    "\n",
    "    num_lbs = len(gt_lb2idxs)\n",
    "    pre = np.zeros(num_lbs)\n",
    "    rec = np.zeros(num_lbs)\n",
    "    gt_num = np.zeros(num_lbs)\n",
    "\n",
    "    for i, gt_idxs in enumerate(gt_lb2idxs.values()):\n",
    "        all_pred_lbs = np.unique(pred_labels[gt_idxs])\n",
    "        gt_num[i] = len(gt_idxs)\n",
    "        for pred_lb in all_pred_lbs:\n",
    "            pred_idxs = pred_lb2idxs[pred_lb]\n",
    "            n = 1.0 * np.intersect1d(gt_idxs, pred_idxs).size\n",
    "            pre[i] += n**2 / len(pred_idxs)\n",
    "            rec[i] += n**2 / gt_num[i]\n",
    "\n",
    "    gt_num = gt_num.sum()\n",
    "    avg_pre = pre.sum() / gt_num\n",
    "    avg_rec = rec.sum() / gt_num\n",
    "    fscore = _compute_fscore(avg_pre, avg_rec)\n",
    "\n",
    "    return avg_pre, avg_rec, fscore\n",
    "\n",
    "\n",
    "def nmi(gt_labels, pred_labels):\n",
    "    return normalized_mutual_info_score(pred_labels, gt_labels)\n",
    "\n",
    "\n",
    "def precision(gt_labels, pred_labels):\n",
    "    return precision_score(gt_labels, pred_labels)\n",
    "\n",
    "\n",
    "def recall(gt_labels, pred_labels):\n",
    "    return recall_score(gt_labels, pred_labels)\n",
    "\n",
    "\n",
    "def accuracy(gt_labels, pred_labels):\n",
    "    return np.mean(gt_labels == pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.694142Z",
     "iopub.status.busy": "2025-06-21T10:11:58.693796Z",
     "iopub.status.idle": "2025-06-21T10:11:58.718665Z",
     "shell.execute_reply": "2025-06-21T10:11:58.718198Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.694115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/knn.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#from utils import Timer\n",
    "\n",
    "from .faiss_search import faiss_search_knn\n",
    "\n",
    "__all__ = [\n",
    "    \"knn_faiss\",\n",
    "    \"knn_faiss_gpu\",\n",
    "    \"fast_knns2spmat\",\n",
    "    \"build_knns\",\n",
    "    \"knns2ordered_nbrs\",\n",
    "]\n",
    "\n",
    "\n",
    "def knns2ordered_nbrs(knns, sort=True):\n",
    "    if isinstance(knns, list):\n",
    "        knns = np.array(knns)\n",
    "    nbrs = knns[:, 0, :].astype(np.int32)\n",
    "    dists = knns[:, 1, :]\n",
    "    if sort:\n",
    "        # sort dists from low to high\n",
    "        nb_idx = np.argsort(dists, axis=1)\n",
    "        idxs = np.arange(nb_idx.shape[0]).reshape(-1, 1)\n",
    "        dists = dists[idxs, nb_idx]\n",
    "        nbrs = nbrs[idxs, nb_idx]\n",
    "    return dists, nbrs\n",
    "\n",
    "\n",
    "def fast_knns2spmat(knns, k, th_sim=0, use_sim=True, fill_value=None):\n",
    "    # convert knns to symmetric sparse matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    eps = 1e-5\n",
    "    n = len(knns)\n",
    "    if isinstance(knns, list):\n",
    "        knns = np.array(knns)\n",
    "    if len(knns.shape) == 2:\n",
    "        # knns saved by hnsw has different shape\n",
    "        n = len(knns)\n",
    "        ndarr = np.ones([n, 2, k])\n",
    "        ndarr[:, 0, :] = -1  # assign unknown dist to 1 and nbr to -1\n",
    "        for i, (nbr, dist) in enumerate(knns):\n",
    "            size = len(nbr)\n",
    "            assert size == len(dist)\n",
    "            ndarr[i, 0, :size] = nbr[:size]\n",
    "            ndarr[i, 1, :size] = dist[:size]\n",
    "        knns = ndarr\n",
    "    nbrs = knns[:, 0, :]\n",
    "    dists = knns[:, 1, :]\n",
    "    assert (\n",
    "        -eps <= dists.min() <= dists.max() <= 1 + eps\n",
    "    ), \"min: {}, max: {}\".format(dists.min(), dists.max())\n",
    "    if use_sim:\n",
    "        sims = 1.0 - dists\n",
    "    else:\n",
    "        sims = dists\n",
    "    if fill_value is not None:\n",
    "        print(\"[fast_knns2spmat] edge fill value:\", fill_value)\n",
    "        sims.fill(fill_value)\n",
    "    row, col = np.where(sims >= th_sim)\n",
    "    # remove the self-loop\n",
    "    idxs = np.where(row != nbrs[row, col])\n",
    "    row = row[idxs]\n",
    "    col = col[idxs]\n",
    "    data = sims[row, col]\n",
    "    col = nbrs[row, col]  # convert to absolute column\n",
    "    assert len(row) == len(col) == len(data)\n",
    "    spmat = csr_matrix((data, (row, col)), shape=(n, n))\n",
    "    return spmat\n",
    "\n",
    "\n",
    "def build_knns(feats, k, knn_method, dump=True):\n",
    "    from utils import Timer\n",
    "\n",
    "    with Timer(\"build index\"):\n",
    "        if knn_method == \"faiss\":\n",
    "            index = knn_faiss(feats, k, omp_num_threads=None)\n",
    "        elif knn_method == \"faiss_gpu\":\n",
    "            index = knn_faiss_gpu(feats, k)\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Only support faiss and faiss_gpu currently ({}).\".format(\n",
    "                    knn_method\n",
    "                )\n",
    "            )\n",
    "        knns = index.get_knns()\n",
    "    return knns\n",
    "\n",
    "\n",
    "class knn:\n",
    "    def __init__(self, feats, k, index_path=\"\", verbose=True):\n",
    "        pass\n",
    "\n",
    "    def filter_by_th(self, i):\n",
    "        th_nbrs = []\n",
    "        th_dists = []\n",
    "        nbrs, dists = self.knns[i]\n",
    "        for n, dist in zip(nbrs, dists):\n",
    "            if 1 - dist < self.th:\n",
    "                continue\n",
    "            th_nbrs.append(n)\n",
    "            th_dists.append(dist)\n",
    "        th_nbrs = np.array(th_nbrs)\n",
    "        th_dists = np.array(th_dists)\n",
    "        return (th_nbrs, th_dists)\n",
    "\n",
    "    def get_knns(self, th=None):\n",
    "        from utils import Timer\n",
    "\n",
    "        if th is None or th <= 0.0:\n",
    "            return self.knns\n",
    "        # TODO: optimize the filtering process by numpy\n",
    "        # nproc = mp.cpu_count()\n",
    "        nproc = 1\n",
    "        with Timer(\n",
    "            \"filter edges by th {} (CPU={})\".format(th, nproc), self.verbose\n",
    "        ):\n",
    "            self.th = th\n",
    "            self.th_knns = []\n",
    "            tot = len(self.knns)\n",
    "            if nproc > 1:\n",
    "                pool = mp.Pool(nproc)\n",
    "                th_knns = list(\n",
    "                    tqdm(pool.imap(self.filter_by_th, range(tot)), total=tot)\n",
    "                )\n",
    "                pool.close()\n",
    "            else:\n",
    "                th_knns = [self.filter_by_th(i) for i in range(tot)]\n",
    "            return th_knns\n",
    "\n",
    "\n",
    "class knn_faiss(knn):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feats,\n",
    "        k,\n",
    "        nprobe=128,\n",
    "        omp_num_threads=None,\n",
    "        rebuild_index=True,\n",
    "        verbose=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        import faiss\n",
    "        from utils import Timer\n",
    "\n",
    "        if omp_num_threads is not None:\n",
    "            faiss.omp_set_num_threads(omp_num_threads)\n",
    "        self.verbose = verbose\n",
    "        with Timer(\"[faiss] build index\", verbose):\n",
    "            feats = feats.astype(\"float32\")\n",
    "            size, dim = feats.shape\n",
    "            index = faiss.IndexFlatIP(dim)\n",
    "            index.add(feats)\n",
    "        with Timer(\"[faiss] query topk {}\".format(k), verbose):\n",
    "            sims, nbrs = index.search(feats, k=k)\n",
    "            self.knns = [\n",
    "                (\n",
    "                    np.array(nbr, dtype=np.int32),\n",
    "                    1 - np.array(sim, dtype=np.float32),\n",
    "                )\n",
    "                for nbr, sim in zip(nbrs, sims)\n",
    "            ]\n",
    "\n",
    "\n",
    "class knn_faiss_gpu(knn):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feats,\n",
    "        k,\n",
    "        nprobe=128,\n",
    "        num_process=4,\n",
    "        is_precise=True,\n",
    "        sort=True,\n",
    "        verbose=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        from utils import Timer\n",
    "\n",
    "        with Timer(\"[faiss_gpu] query topk {}\".format(k), verbose):\n",
    "            dists, nbrs = faiss_search_knn(\n",
    "                feats,\n",
    "                k=k,\n",
    "                nprobe=nprobe,\n",
    "                num_process=num_process,\n",
    "                is_precise=is_precise,\n",
    "                sort=sort,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            self.knns = [\n",
    "                (\n",
    "                    np.array(nbr, dtype=np.int32),\n",
    "                    np.array(dist, dtype=np.float32),\n",
    "                )\n",
    "                for nbr, dist in zip(nbrs, dists)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.719822Z",
     "iopub.status.busy": "2025-06-21T10:11:58.719557Z",
     "iopub.status.idle": "2025-06-21T10:11:58.732184Z",
     "shell.execute_reply": "2025-06-21T10:11:58.731608Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.719788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/faiss_search.py\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .faiss_gpu import faiss_search_approx_knn\n",
    "\n",
    "__all__ = [\"faiss_search_knn\"]\n",
    "\n",
    "\n",
    "def precise_dist(feat, nbrs, num_process=4, sort=True, verbose=False):\n",
    "    import torch\n",
    "\n",
    "    feat_share = torch.from_numpy(feat).share_memory_()\n",
    "    nbrs_share = torch.from_numpy(nbrs).share_memory_()\n",
    "    dist_share = torch.zeros_like(nbrs_share).float().share_memory_()\n",
    "\n",
    "    precise_dist_share_mem(\n",
    "        feat_share,\n",
    "        nbrs_share,\n",
    "        dist_share,\n",
    "        num_process=num_process,\n",
    "        sort=sort,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    del feat_share\n",
    "    gc.collect()\n",
    "    return dist_share.numpy(), nbrs_share.numpy()\n",
    "\n",
    "\n",
    "def precise_dist_share_mem(\n",
    "    feat,\n",
    "    nbrs,\n",
    "    dist,\n",
    "    num_process=16,\n",
    "    sort=True,\n",
    "    process_unit=4000,\n",
    "    verbose=False,\n",
    "):\n",
    "    from torch import multiprocessing as mp\n",
    "\n",
    "    num, _ = feat.shape\n",
    "    num_per_proc = int(num / num_process) + 1\n",
    "\n",
    "    for pi in range(num_process):\n",
    "        sid = pi * num_per_proc\n",
    "        eid = min(sid + num_per_proc, num)\n",
    "\n",
    "        kwargs = {\n",
    "            \"feat\": feat,\n",
    "            \"nbrs\": nbrs,\n",
    "            \"dist\": dist,\n",
    "            \"sid\": sid,\n",
    "            \"eid\": eid,\n",
    "            \"sort\": sort,\n",
    "            \"process_unit\": process_unit,\n",
    "            \"verbose\": verbose,\n",
    "        }\n",
    "        bmm(**kwargs)\n",
    "\n",
    "\n",
    "def bmm(\n",
    "    feat, nbrs, dist, sid, eid, sort=True, process_unit=4000, verbose=False\n",
    "):\n",
    "    import torch\n",
    "\n",
    "    _, cols = dist.shape\n",
    "    batch_sim = torch.zeros((eid - sid, cols), dtype=torch.float32)\n",
    "    for s in tqdm(\n",
    "        range(sid, eid, process_unit), desc=\"bmm\", disable=not verbose\n",
    "    ):\n",
    "        e = min(eid, s + process_unit)\n",
    "        query = feat[s:e].unsqueeze(1)\n",
    "        gallery = feat[nbrs[s:e]].permute(0, 2, 1)\n",
    "        batch_sim[s - sid : e - sid] = torch.clamp(\n",
    "            torch.bmm(query, gallery).view(-1, cols), 0.0, 1.0\n",
    "        )\n",
    "\n",
    "    if sort:\n",
    "        sort_unit = int(1e6)\n",
    "        batch_nbr = nbrs[sid:eid]\n",
    "        for s in range(0, batch_sim.shape[0], sort_unit):\n",
    "            e = min(s + sort_unit, eid)\n",
    "            batch_sim[s:e], indices = torch.sort(\n",
    "                batch_sim[s:e], descending=True\n",
    "            )\n",
    "            batch_nbr[s:e] = torch.gather(batch_nbr[s:e], 1, indices)\n",
    "        nbrs[sid:eid] = batch_nbr\n",
    "    dist[sid:eid] = 1.0 - batch_sim\n",
    "\n",
    "\n",
    "def faiss_search_knn(\n",
    "    feat,\n",
    "    k,\n",
    "    nprobe=128,\n",
    "    num_process=4,\n",
    "    is_precise=True,\n",
    "    sort=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    dists, nbrs = faiss_search_approx_knn(\n",
    "        query=feat, target=feat, k=k, nprobe=nprobe, verbose=verbose\n",
    "    )\n",
    "\n",
    "    if is_precise:\n",
    "        print(\"compute precise dist among k={} nearest neighbors\".format(k))\n",
    "        dists, nbrs = precise_dist(\n",
    "            feat, nbrs, num_process=num_process, sort=sort, verbose=verbose\n",
    "        )\n",
    "\n",
    "    return dists, nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.735128Z",
     "iopub.status.busy": "2025-06-21T10:11:58.734906Z",
     "iopub.status.idle": "2025-06-21T10:11:58.744648Z",
     "shell.execute_reply": "2025-06-21T10:11:58.744003Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.735107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/faiss_gpu.py\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "__all__ = [\"faiss_search_approx_knn\"]\n",
    "\n",
    "\n",
    "class faiss_index_wrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target,\n",
    "        nprobe=128,\n",
    "        index_factory_str=None,\n",
    "        verbose=False,\n",
    "        mode=\"proxy\",\n",
    "        using_gpu=True,\n",
    "    ):\n",
    "        self._res_list = []\n",
    "\n",
    "        num_gpu = faiss.get_num_gpus()\n",
    "        print(\"[faiss gpu] #GPU: {}\".format(num_gpu))\n",
    "\n",
    "        size, dim = target.shape\n",
    "        assert size > 0, \"size: {}\".format(size)\n",
    "        index_factory_str = (\n",
    "            \"IVF{},PQ{}\".format(min(8192, 16 * round(np.sqrt(size))), 32)\n",
    "            if index_factory_str is None\n",
    "            else index_factory_str\n",
    "        )\n",
    "        cpu_index = faiss.index_factory(dim, index_factory_str)\n",
    "        cpu_index.nprobe = nprobe\n",
    "\n",
    "        if mode == \"proxy\":\n",
    "            co = faiss.GpuClonerOptions()\n",
    "            co.useFloat16 = True\n",
    "            co.usePrecomputed = False\n",
    "\n",
    "            index = faiss.IndexProxy()\n",
    "            for i in range(num_gpu):\n",
    "                res = faiss.StandardGpuResources()\n",
    "                self._res_list.append(res)\n",
    "                sub_index = (\n",
    "                    faiss.index_cpu_to_gpu(res, i, cpu_index, co)\n",
    "                    if using_gpu\n",
    "                    else cpu_index\n",
    "                )\n",
    "                index.addIndex(sub_index)\n",
    "        elif mode == \"shard\":\n",
    "            co = faiss.GpuMultipleClonerOptions()\n",
    "            co.useFloat16 = True\n",
    "            co.usePrecomputed = False\n",
    "            co.shard = True\n",
    "            index = faiss.index_cpu_to_all_gpus(cpu_index, co, ngpu=num_gpu)\n",
    "        else:\n",
    "            raise KeyError(\"Unknown index mode\")\n",
    "\n",
    "        index = faiss.IndexIDMap(index)\n",
    "        index.verbose = verbose\n",
    "\n",
    "        # get nlist to decide how many samples used for training\n",
    "        nlist = int(\n",
    "            float(\n",
    "                [\n",
    "                    item\n",
    "                    for item in index_factory_str.split(\",\")\n",
    "                    if \"IVF\" in item\n",
    "                ][0].replace(\"IVF\", \"\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # training\n",
    "        if not index.is_trained:\n",
    "            indexes_sample_for_train = np.random.randint(0, size, nlist * 256)\n",
    "            index.train(target[indexes_sample_for_train])\n",
    "\n",
    "        # add with ids\n",
    "        target_ids = np.arange(0, size)\n",
    "        index.add_with_ids(target, target_ids)\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, *args, **kargs):\n",
    "        return self.index.search(*args, **kargs)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.index.reset()\n",
    "        del self.index\n",
    "        for res in self._res_list:\n",
    "            del res\n",
    "\n",
    "\n",
    "def batch_search(index, query, k, bs, verbose=False):\n",
    "    n = len(query)\n",
    "    dists = np.zeros((n, k), dtype=np.float32)\n",
    "    nbrs = np.zeros((n, k), dtype=np.int64)\n",
    "\n",
    "    for sid in tqdm(\n",
    "        range(0, n, bs), desc=\"faiss searching...\", disable=not verbose\n",
    "    ):\n",
    "        eid = min(n, sid + bs)\n",
    "        dists[sid:eid], nbrs[sid:eid] = index.search(query[sid:eid], k)\n",
    "    return dists, nbrs\n",
    "\n",
    "\n",
    "def faiss_search_approx_knn(\n",
    "    query,\n",
    "    target,\n",
    "    k,\n",
    "    nprobe=128,\n",
    "    bs=int(1e6),\n",
    "    index_factory_str=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    index = faiss_index_wrapper(\n",
    "        target,\n",
    "        nprobe=nprobe,\n",
    "        index_factory_str=index_factory_str,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    dists, nbrs = batch_search(index, query, k=k, bs=bs, verbose=verbose)\n",
    "\n",
    "    del index\n",
    "    gc.collect()\n",
    "    return dists, nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.745844Z",
     "iopub.status.busy": "2025-06-21T10:11:58.745408Z",
     "iopub.status.idle": "2025-06-21T10:11:58.758609Z",
     "shell.execute_reply": "2025-06-21T10:11:58.758022Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.745828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/density.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "__all__ = [\n",
    "    \"density_estimation\",\n",
    "    \"density_to_peaks\",\n",
    "    \"density_to_peaks_vectorize\",\n",
    "]\n",
    "\n",
    "\n",
    "def density_estimation(dists, nbrs, labels, **kwargs):\n",
    "    \"\"\"use supervised density defined on neigborhood\"\"\"\n",
    "    num, k_knn = dists.shape\n",
    "    conf = np.ones((num,), dtype=np.float32)\n",
    "    ind_array = labels[nbrs] == np.expand_dims(labels, 1).repeat(k_knn, 1)\n",
    "    pos = ((1 - dists[:, 1:]) * ind_array[:, 1:]).sum(1)\n",
    "    neg = ((1 - dists[:, 1:]) * (1 - ind_array[:, 1:])).sum(1)\n",
    "    conf = (pos - neg) * conf\n",
    "    conf /= k_knn - 1\n",
    "    return conf\n",
    "\n",
    "\n",
    "def density_to_peaks_vectorize(dists, nbrs, density, max_conn=1, name=\"\"):\n",
    "    # just calculate 1 connectivity\n",
    "    assert dists.shape[0] == density.shape[0]\n",
    "    assert dists.shape == nbrs.shape\n",
    "\n",
    "    num, k = dists.shape\n",
    "\n",
    "    if name == \"gcn_feat\":\n",
    "        include_mask = nbrs != np.arange(0, num).reshape(-1, 1)\n",
    "        secondary_mask = (\n",
    "            np.sum(include_mask, axis=1) == k\n",
    "        )  # TODO: the condition == k should not happen as distance to the node self should be smallest, check for numerical stability; TODO: make top M instead of only supporting top 1\n",
    "        include_mask[secondary_mask, -1] = False\n",
    "        nbrs_exclude_self = nbrs[include_mask].reshape(-1, k - 1)  # (V, 79)\n",
    "        dists_exclude_self = dists[include_mask].reshape(-1, k - 1)  # (V, 79)\n",
    "    else:\n",
    "        include_mask = nbrs != np.arange(0, num).reshape(-1, 1)\n",
    "        nbrs_exclude_self = nbrs[include_mask].reshape(-1, k - 1)  # (V, 79)\n",
    "        dists_exclude_self = dists[include_mask].reshape(-1, k - 1)  # (V, 79)\n",
    "\n",
    "    compare_map = density[nbrs_exclude_self] > density.reshape(-1, 1)\n",
    "    peak_index = np.argmax(np.where(compare_map, 1, 0), axis=1)  # (V,)\n",
    "    compare_map_sum = np.sum(compare_map.cpu().data.numpy(), axis=1)  # (V,)\n",
    "\n",
    "    dist2peak = {\n",
    "        i: []\n",
    "        if compare_map_sum[i] == 0\n",
    "        else [dists_exclude_self[i, peak_index[i]]]\n",
    "        for i in range(num)\n",
    "    }\n",
    "    peaks = {\n",
    "        i: []\n",
    "        if compare_map_sum[i] == 0\n",
    "        else [nbrs_exclude_self[i, peak_index[i]]]\n",
    "        for i in range(num)\n",
    "    }\n",
    "\n",
    "    return dist2peak, peaks\n",
    "\n",
    "\n",
    "def density_to_peaks(dists, nbrs, density, max_conn=1, sort=\"dist\"):\n",
    "    # Note that dists has been sorted in ascending order\n",
    "    assert dists.shape[0] == density.shape[0]\n",
    "    assert dists.shape == nbrs.shape\n",
    "\n",
    "    num, _ = dists.shape\n",
    "    dist2peak = {i: [] for i in range(num)}\n",
    "    peaks = {i: [] for i in range(num)}\n",
    "\n",
    "    for i, nbr in tqdm(enumerate(nbrs)):\n",
    "        nbr_conf = density[nbr]\n",
    "        for j, c in enumerate(nbr_conf):\n",
    "            nbr_idx = nbr[j]\n",
    "            if i == nbr_idx or c <= density[i]:\n",
    "                continue\n",
    "            dist2peak[i].append(dists[i, j])\n",
    "            peaks[i].append(nbr_idx)\n",
    "            if len(dist2peak[i]) >= max_conn:\n",
    "                break\n",
    "\n",
    "    return dist2peak, peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.759634Z",
     "iopub.status.busy": "2025-06-21T10:11:58.759405Z",
     "iopub.status.idle": "2025-06-21T10:11:58.772669Z",
     "shell.execute_reply": "2025-06-21T10:11:58.772133Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.759614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/deduce.py\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import mixture\n",
    "\n",
    "from .density import density_to_peaks, density_to_peaks_vectorize\n",
    "\n",
    "__all__ = [\n",
    "    \"peaks_to_labels\",\n",
    "    \"edge_to_connected_graph\",\n",
    "    \"decode\",\n",
    "    \"build_next_level\",\n",
    "    \"build_next_level_mod6\",\n",
    "    \"build_next_level_mod7\",\n",
    "]\n",
    "\n",
    "\n",
    "def _find_parent(parent, u):\n",
    "    idx = []\n",
    "    # parent is a fixed point\n",
    "    while u != parent[u]:\n",
    "        idx.append(u)\n",
    "        u = parent[u]\n",
    "    for i in idx:\n",
    "        parent[i] = u\n",
    "    return u\n",
    "\n",
    "\n",
    "def edge_to_connected_graph(edges, num):\n",
    "    parent = list(range(num))\n",
    "    for u, v in edges:\n",
    "        p_u = _find_parent(parent, u)\n",
    "        p_v = _find_parent(parent, v)\n",
    "        parent[p_u] = p_v\n",
    "\n",
    "    for i in range(num):\n",
    "        parent[i] = _find_parent(parent, i)\n",
    "    remap = {}\n",
    "    uf = np.unique(np.array(parent))\n",
    "    for i, f in enumerate(uf):\n",
    "        remap[f] = i\n",
    "    cluster_id = np.array([remap[f] for f in parent])\n",
    "    return cluster_id\n",
    "\n",
    "\n",
    "def peaks_to_edges(peaks, dist2peak, tau):\n",
    "    edges = []\n",
    "    for src in peaks:\n",
    "        dsts = peaks[src]\n",
    "        dists = dist2peak[src]\n",
    "        for dst, dist in zip(dsts, dists):\n",
    "            if src == dst or dist >= 1 - tau:\n",
    "                continue\n",
    "            edges.append([src, dst])\n",
    "    return edges\n",
    "\n",
    "\n",
    "def peaks_to_labels(peaks, dist2peak, tau, inst_num):\n",
    "    edges = peaks_to_edges(peaks, dist2peak, tau)\n",
    "    pred_labels = edge_to_connected_graph(edges, inst_num)\n",
    "    return pred_labels, edges\n",
    "\n",
    "\n",
    "def get_dists(g, nbrs, use_gt):\n",
    "    k = nbrs.shape[1]\n",
    "    src_id = nbrs[:, 1:].reshape(-1)\n",
    "    dst_id = nbrs[:, 0].repeat(k - 1)\n",
    "    eids = g.edge_ids(src_id, dst_id)\n",
    "    if use_gt:\n",
    "        new_dists = (\n",
    "            (1 - g.edata[\"labels_edge\"][eids]).reshape(-1, k - 1).float()\n",
    "        )\n",
    "    else:\n",
    "        new_dists = g.edata[\"prob_conn\"][eids, 0].reshape(-1, k - 1)\n",
    "    ind = torch.argsort(new_dists, 1)\n",
    "    offset = torch.LongTensor(\n",
    "        (nbrs[:, 0] * (k - 1)).repeat(k - 1).reshape(-1, k - 1)\n",
    "    ).to(g.device)\n",
    "    ind = ind + offset\n",
    "    nbrs = torch.LongTensor(nbrs).to(g.device)\n",
    "    new_nbrs = torch.take(nbrs[:, 1:], ind)\n",
    "    new_dists = torch.cat(\n",
    "        [torch.zeros((new_dists.shape[0], 1)).to(g.device), new_dists], dim=1\n",
    "    )\n",
    "    new_nbrs = torch.cat(\n",
    "        [torch.arange(new_nbrs.shape[0]).view(-1, 1).to(g.device), new_nbrs],\n",
    "        dim=1,\n",
    "    )\n",
    "    return new_nbrs.cpu().detach().numpy(), new_dists.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_edge_dist(g, threshold):\n",
    "    if threshold == \"prob\":\n",
    "        return g.edata[\"prob_conn\"][:, 0]\n",
    "    return 1 - g.edata[\"raw_affine\"]\n",
    "\n",
    "\n",
    "def tree_generation(ng):\n",
    "    ng.ndata[\"keep_eid\"] = torch.zeros(ng.num_nodes()).long() - 1\n",
    "\n",
    "    def message_func(edges):\n",
    "        return {\"mval\": edges.data[\"edge_dist\"], \"meid\": edges.data[dgl.EID]}\n",
    "\n",
    "    def reduce_func(nodes):\n",
    "        ind = torch.min(nodes.mailbox[\"mval\"], dim=1)[1]\n",
    "        keep_eid = nodes.mailbox[\"meid\"].gather(1, ind.view(-1, 1))\n",
    "        return {\"keep_eid\": keep_eid[:, 0]}\n",
    "\n",
    "    node_order = dgl.traversal.topological_nodes_generator(ng)\n",
    "    ng.prop_nodes(node_order, message_func, reduce_func)\n",
    "    eids = ng.ndata[\"keep_eid\"]\n",
    "    eids = eids[eids > -1]\n",
    "    edges = ng.find_edges(eids)\n",
    "    treeg = dgl.graph(edges, num_nodes=ng.num_nodes())\n",
    "    return treeg\n",
    "\n",
    "\n",
    "def peak_propogation(treeg):\n",
    "    treeg.ndata[\"pred_labels\"] = torch.zeros(treeg.num_nodes()).long() - 1\n",
    "    peaks = torch.where(treeg.in_degrees() == 0)[0].cpu().numpy()\n",
    "    treeg.ndata[\"pred_labels\"][peaks] = torch.arange(peaks.shape[0])\n",
    "\n",
    "    def message_func(edges):\n",
    "        return {\"mlb\": edges.src[\"pred_labels\"]}\n",
    "\n",
    "    def reduce_func(nodes):\n",
    "        return {\"pred_labels\": nodes.mailbox[\"mlb\"][:, 0]}\n",
    "\n",
    "    node_order = dgl.traversal.topological_nodes_generator(treeg)\n",
    "    treeg.prop_nodes(node_order, message_func, reduce_func)\n",
    "    pred_labels = treeg.ndata[\"pred_labels\"].cpu().numpy()\n",
    "    return peaks, pred_labels\n",
    "\n",
    "\n",
    "def decode(\n",
    "    g,\n",
    "    tau,\n",
    "    threshold,\n",
    "    use_gt,\n",
    "    ids=None,\n",
    "    global_edges=None,\n",
    "    global_num_nodes=None,\n",
    "    global_peaks=None,\n",
    "):\n",
    "    # Edge filtering with tau and density\n",
    "    den_key = \"density\" if use_gt else \"pred_den\"\n",
    "    g = g.local_var()\n",
    "    g.edata[\"edge_dist\"] = get_edge_dist(g, threshold)\n",
    "    g.apply_edges(\n",
    "        lambda edges: {\n",
    "            \"keep\": (edges.src[den_key] > edges.dst[den_key]).long()\n",
    "            * (edges.data[\"edge_dist\"] < 1 - tau).long()\n",
    "        }\n",
    "    )\n",
    "    eids = torch.where(g.edata[\"keep\"] == 0)[0]\n",
    "    ng = dgl.remove_edges(g, eids)\n",
    "\n",
    "    # Tree generation\n",
    "    ng.edata[dgl.EID] = torch.arange(ng.num_edges())\n",
    "    treeg = tree_generation(ng)\n",
    "    # Label propogation\n",
    "    peaks, pred_labels = peak_propogation(treeg)\n",
    "\n",
    "    if ids is None:\n",
    "        return pred_labels, peaks\n",
    "\n",
    "    # Merge with previous layers\n",
    "    src, dst = treeg.edges()\n",
    "    new_global_edges = (\n",
    "        global_edges[0] + ids[src.numpy()].tolist(),\n",
    "        global_edges[1] + ids[dst.numpy()].tolist(),\n",
    "    )\n",
    "    global_treeg = dgl.graph(new_global_edges, num_nodes=global_num_nodes)\n",
    "    global_peaks, global_pred_labels = peak_propogation(global_treeg)\n",
    "    return (\n",
    "        pred_labels,\n",
    "        peaks,\n",
    "        new_global_edges,\n",
    "        global_pred_labels,\n",
    "        global_peaks,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_next_level(\n",
    "    features, labels, peaks, global_features, global_pred_labels, global_peaks\n",
    "):\n",
    "    global_peak_to_label = global_pred_labels[global_peaks]\n",
    "    global_label_to_peak = np.zeros_like(global_peak_to_label)\n",
    "    for i, pl in enumerate(global_peak_to_label):\n",
    "        global_label_to_peak[pl] = i\n",
    "    cluster_ind = np.split(\n",
    "        np.argsort(global_pred_labels),\n",
    "        np.unique(np.sort(global_pred_labels), return_index=True)[1][1:],\n",
    "    )\n",
    "    cluster_features = np.zeros((len(peaks), global_features.shape[1]))\n",
    "    for pi in range(len(peaks)):\n",
    "        cluster_features[global_label_to_peak[pi], :] = np.mean(\n",
    "            global_features[cluster_ind[pi], :], axis=0\n",
    "        )\n",
    "    features = features[peaks]\n",
    "    labels = labels[peaks]\n",
    "    return features, labels, cluster_features\n",
    "\n",
    "################################ MODIFICA 6\n",
    "def build_next_level_mod6(\n",
    "    features, labels, peaks, global_features, global_pred_labels, global_peaks\n",
    "):\n",
    "    global_peak_to_label = global_pred_labels[global_peaks]\n",
    "    global_label_to_peak = np.zeros_like(global_peak_to_label)\n",
    "    for i, pl in enumerate(global_peak_to_label):\n",
    "        global_label_to_peak[pl] = i\n",
    "    \n",
    "    # Invece di aggregare tramite media, usiamo direttamente le features dei peak nodes\n",
    "    cluster_features = np.zeros((len(peaks), global_features.shape[1]))\n",
    "    for pi in range(len(peaks)):\n",
    "        peak_node_idx = global_peaks[global_label_to_peak[pi]]\n",
    "        cluster_features[global_label_to_peak[pi], :] = global_features[peak_node_idx, :]\n",
    "    \n",
    "    features = features[peaks]\n",
    "    labels = labels[peaks]\n",
    "    return features, labels, cluster_features\n",
    "\n",
    "############################### MODIFICA 7\n",
    "def build_next_level_mod7(\n",
    "    features, labels, peaks, global_features, global_pred_labels, global_peaks, size_threshold=10000\n",
    "):\n",
    "    global_peak_to_label = global_pred_labels[global_peaks]\n",
    "    global_label_to_peak = np.zeros_like(global_peak_to_label)\n",
    "    for i, pl in enumerate(global_peak_to_label):\n",
    "        global_label_to_peak[pl] = i\n",
    "    \n",
    "    cluster_ind = np.split(\n",
    "        np.argsort(global_pred_labels),\n",
    "        np.unique(np.sort(global_pred_labels), return_index=True)[1][1:],\n",
    "    )\n",
    "    cluster_features = np.zeros((len(peaks), global_features.shape[1]))\n",
    "    \n",
    "    for pi in range(len(peaks)):\n",
    "        cluster_indices = cluster_ind[pi]\n",
    "        cluster_size = len(cluster_indices)\n",
    "        peak_node_idx = global_peaks[global_label_to_peak[pi]]\n",
    "        \n",
    "        if cluster_size <= size_threshold:\n",
    "            # Per cluster piccoli usiamo solo il peak\n",
    "            cluster_features[global_label_to_peak[pi], :] = global_features[peak_node_idx, :]\n",
    "        else:\n",
    "            # Per cluster grandi uiamo la media\n",
    "            cluster_features[global_label_to_peak[pi], :] = np.mean(\n",
    "                global_features[cluster_indices, :], axis=0\n",
    "            )\n",
    "    \n",
    "    features = features[peaks]\n",
    "    labels = labels[peaks]\n",
    "    return features, labels, cluster_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.773569Z",
     "iopub.status.busy": "2025-06-21T10:11:58.773350Z",
     "iopub.status.idle": "2025-06-21T10:11:58.785597Z",
     "shell.execute_reply": "2025-06-21T10:11:58.784879Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.773549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/adjacency.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file re-uses implementation from https://github.com/yl-1993/learn-to-cluster\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    # if rowsum <= 0, keep its previous value\n",
    "    rowsum[rowsum <= 0] = 1\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.0\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx, r_inv\n",
    "\n",
    "\n",
    "def sparse_mx_to_indices_values(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    values = sparse_mx.data\n",
    "    shape = np.array(sparse_mx.shape)\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.786483Z",
     "iopub.status.busy": "2025-06-21T10:11:58.786270Z",
     "iopub.status.idle": "2025-06-21T10:11:58.799708Z",
     "shell.execute_reply": "2025-06-21T10:11:58.799010Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.786460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/evaluate.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/p')  # root del progetto\n",
    "sys.path.append('/kaggle/working/p/clustering-benchmark')\n",
    "\n",
    "import numpy as np\n",
    "from clustering_benchmark import ClusteringBenchmark\n",
    "#from utils import metrics, TextColors, Timer\n",
    "\n",
    "def _read_meta(fn):\n",
    "    labels = list()\n",
    "    lb_set = set()\n",
    "    with open(fn) as f:\n",
    "        for lb in f.readlines():\n",
    "            lb = int(lb.strip())\n",
    "            labels.append(lb)\n",
    "            lb_set.add(lb)\n",
    "    return np.array(labels), lb_set\n",
    "\n",
    "\n",
    "def evaluate(gt_labels, pred_labels, metric=\"pairwise\"):\n",
    "    from utils import metrics, TextColors, Timer  \n",
    "    if isinstance(gt_labels, str) and isinstance(pred_labels, str):\n",
    "        print(\"[gt_labels] {}\".format(gt_labels))\n",
    "        print(\"[pred_labels] {}\".format(pred_labels))\n",
    "        gt_labels, gt_lb_set = _read_meta(gt_labels)\n",
    "        pred_labels, pred_lb_set = _read_meta(pred_labels)\n",
    "\n",
    "        print(\n",
    "            \"#inst: gt({}) vs pred({})\".format(len(gt_labels), len(pred_labels))\n",
    "        )\n",
    "        print(\n",
    "            \"#cls: gt({}) vs pred({})\".format(len(gt_lb_set), len(pred_lb_set))\n",
    "        )\n",
    "\n",
    "    metric_func = metrics.__dict__[metric]\n",
    "\n",
    "    with Timer(\n",
    "        \"evaluate with {}{}{}\".format(TextColors.FATAL, metric, TextColors.ENDC)\n",
    "    ):\n",
    "        result = metric_func(gt_labels, pred_labels)\n",
    "    if isinstance(result, float):\n",
    "        print(\n",
    "            \"{}{}: {:.4f}{}\".format(\n",
    "                TextColors.OKGREEN, metric, result, TextColors.ENDC\n",
    "            )\n",
    "        )\n",
    "        return f\"{metric},{result:.4f}\"\n",
    "    else:\n",
    "        ave_pre, ave_rec, fscore = result\n",
    "        print(\n",
    "            \"{}ave_pre: {:.4f}, ave_rec: {:.4f}, fscore: {:.4f}{}\".format(\n",
    "                TextColors.OKGREEN, ave_pre, ave_rec, fscore, TextColors.ENDC\n",
    "            )\n",
    "        )\n",
    "        return f\"{metric}_ave_pre,{ave_pre:.4f}\\n{metric}_ave_rec,{ave_rec:.4f}\\n{metric}_fscore,{fscore:.4f}\"\n",
    "\n",
    "\n",
    "def evaluation(pred_labels, labels, metrics,output_csv_path=\"evaluation_metrics.csv\"):\n",
    "    print(\"==> evaluation\")\n",
    "    # pred_labels = g.ndata['pred_labels'].cpu().numpy()\n",
    "    max_cluster = np.max(pred_labels)\n",
    "    # gt_labels_all = g.ndata['labels'].cpu().numpy()\n",
    "    gt_labels_all = labels\n",
    "    pred_labels_all = pred_labels\n",
    "    metric_list = metrics.split(\",\")\n",
    "    \n",
    "    csv_lines = [\"Metric,Value\"]\n",
    "\n",
    "    for metric in metric_list:\n",
    "        metric_output  = evaluate(gt_labels_all, pred_labels_all, metric)\n",
    "        if '\\n' in metric_output:\n",
    "            csv_lines.extend(metric_output.split('\\n'))\n",
    "        else:\n",
    "            csv_lines.append(metric_output)\n",
    "            \n",
    "    # H and C-scores\n",
    "    gt_dict = {}\n",
    "    pred_dict = {}\n",
    "    for i in range(len(gt_labels_all)):\n",
    "        gt_dict[str(i)] = gt_labels_all[i]\n",
    "        pred_dict[str(i)] = pred_labels_all[i]\n",
    "    bm = ClusteringBenchmark(gt_dict)\n",
    "    scores = bm.evaluate_vmeasure(pred_dict)\n",
    "    fmi_scores = bm.evaluate_fowlkes_mallows_score(pred_dict)\n",
    "\n",
    "    # Esempio di print\n",
    "    #{'#gt clusters': 2452, '#pred clusters': 39732, 'h-score': 0.8615836636450002, 'c-score': 0.6993937747345866, 'v-meansure': 0.7720627293522723}\n",
    "    csv_lines.append(f\"scores_h_score,{scores['h-score']:.4f}\")\n",
    "    csv_lines.append(f\"scores_c_score,{scores['c-score']:.4f}\")\n",
    "    csv_lines.append(f\"scores_v_score,{scores['v-meansure']:.4f}\")\n",
    "    csv_lines.append(f\"gt clusters,{scores['#gt clusters']:.4f}\")\n",
    "    csv_lines.append(f\"pred clusters,{scores['#pred clusters']:.4f}\")\n",
    "    \n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        for line in csv_lines:\n",
    "            csvfile.write(line + '\\n')\n",
    "        \n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.800596Z",
     "iopub.status.busy": "2025-06-21T10:11:58.800359Z",
     "iopub.status.idle": "2025-06-21T10:11:58.813722Z",
     "shell.execute_reply": "2025-06-21T10:11:58.813027Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.800576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/utils/__init__.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from .adjacency import *\n",
    "from .deduce import *\n",
    "from .density import *\n",
    "from .evaluate import *\n",
    "from .faiss_gpu import faiss_search_approx_knn\n",
    "from .faiss_search import faiss_search_knn\n",
    "from .knn import *\n",
    "from .metrics import *\n",
    "from .misc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARTELLA MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.815087Z",
     "iopub.status.busy": "2025-06-21T10:11:58.814529Z",
     "iopub.status.idle": "2025-06-21T10:11:58.828293Z",
     "shell.execute_reply": "2025-06-21T10:11:58.827702Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.815065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/models/lander.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from .focal_loss import FocalLoss\n",
    "from .graphconv import GraphConv\n",
    "\n",
    "\n",
    "class LANDER(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        nhid,\n",
    "        num_conv=4,\n",
    "        dropout=0,\n",
    "        use_GAT=True,\n",
    "        K=1,\n",
    "        balance=False,\n",
    "        use_cluster_feat=True,\n",
    "        use_focal_loss=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(LANDER, self).__init__()\n",
    "        nhid_half = int(nhid / 2)\n",
    "        self.use_cluster_feat = use_cluster_feat\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "\n",
    "        if self.use_cluster_feat:\n",
    "            self.feature_dim = feature_dim * 2\n",
    "        else:\n",
    "            self.feature_dim = feature_dim\n",
    "\n",
    "        input_dim = (feature_dim, nhid, nhid, nhid_half)\n",
    "        output_dim = (nhid, nhid, nhid_half, nhid_half)\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.conv.append(GraphConv(self.feature_dim, nhid, dropout, use_GAT, K))\n",
    "        for i in range(1, num_conv):\n",
    "            self.conv.append(\n",
    "                GraphConv(input_dim[i], output_dim[i], dropout, use_GAT, K)\n",
    "            )\n",
    "\n",
    "        self.src_mlp = nn.Linear(output_dim[num_conv - 1], nhid_half)\n",
    "        self.dst_mlp = nn.Linear(output_dim[num_conv - 1], nhid_half)\n",
    "\n",
    "\n",
    "        ###################################### OG\n",
    "        self.classifier_conn = nn.Sequential(\n",
    "            nn.PReLU(nhid_half),\n",
    "            nn.Linear(nhid_half, nhid_half),\n",
    "            nn.PReLU(nhid_half),\n",
    "            nn.Linear(nhid_half, 2),\n",
    "        )\n",
    "        \"\"\"\n",
    "        ##################################### 4 MODIFICA\n",
    "        self.classifier_conn_4 = nn.Sequential(\n",
    "            nn.PReLU(nhid_half),\n",
    "            nn.Linear(nhid_half, nhid_half),\n",
    "            nn.BatchNorm1d(nhid_half),\n",
    "            nn.PReLU(nhid_half),\n",
    "            nn.Linear(nhid_half, nhid_half),\n",
    "            nn.BatchNorm1d(nhid_half),\n",
    "            nn.PReLU(nhid_half),             \n",
    "            nn.Linear(nhid_half, 2),\n",
    "        )\n",
    "\n",
    "        ##################################### 5 MODIFICA\n",
    "        self.classifier_conn_5 = nn.Sequential(\n",
    "            nn.PReLU(2*nhid_half),\n",
    "            nn.Linear(2*nhid_half, nhid_half),\n",
    "            nn.BatchNorm1d(nhid_half),\n",
    "            nn.PReLU(nhid_half),             \n",
    "            nn.Linear(nhid_half, 2),\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.use_focal_loss:\n",
    "            self.loss_conn = FocalLoss(2)\n",
    "        else:\n",
    "            self.loss_conn = nn.CrossEntropyLoss()\n",
    "        self.loss_den = nn.MSELoss()\n",
    "\n",
    "        self.balance = balance\n",
    "\n",
    "    def pred_conn(self, edges):\n",
    "        src_feat = self.src_mlp(edges.src[\"conv_features\"])\n",
    "        dst_feat = self.dst_mlp(edges.dst[\"conv_features\"])\n",
    "        ################  OG\n",
    "        pred_conn = self.classifier_conn(src_feat + dst_feat)\n",
    "        \n",
    "        ################ 5 MODIFICA\n",
    "        #combined_feat = torch.cat([src_feat, dst_feat], dim=-1) #\n",
    "        #pred_conn = self.classifier_conn(combined_feat) #\n",
    "        \n",
    "        return {\"pred_conn\": pred_conn}\n",
    "\n",
    "    def pred_den_msg(self, edges):\n",
    "        prob = edges.data[\"prob_conn\"]\n",
    "        res = edges.data[\"raw_affine\"] * (prob[:, 1] - prob[:, 0])\n",
    "        return {\"pred_den_msg\": res}\n",
    "\n",
    "    def forward(self, bipartites):\n",
    "        if isinstance(bipartites, dgl.DGLGraph):\n",
    "            bipartites = [bipartites] * len(self.conv)\n",
    "            if self.use_cluster_feat:\n",
    "                neighbor_x = torch.cat(\n",
    "                    [\n",
    "                        bipartites[0].ndata[\"features\"],\n",
    "                        bipartites[0].ndata[\"cluster_features\"],\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                neighbor_x = bipartites[0].ndata[\"features\"]\n",
    "\n",
    "            for i in range(len(self.conv)):\n",
    "                neighbor_x = self.conv[i](bipartites[i], neighbor_x)\n",
    "\n",
    "            output_bipartite = bipartites[-1]\n",
    "            output_bipartite.ndata[\"conv_features\"] = neighbor_x\n",
    "        else:\n",
    "            if self.use_cluster_feat:\n",
    "                neighbor_x_src = torch.cat(\n",
    "                    [\n",
    "                        bipartites[0].srcdata[\"features\"],\n",
    "                        bipartites[0].srcdata[\"cluster_features\"],\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                center_x_src = torch.cat(\n",
    "                    [\n",
    "                        bipartites[1].srcdata[\"features\"],\n",
    "                        bipartites[1].srcdata[\"cluster_features\"],\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                neighbor_x_src = bipartites[0].srcdata[\"features\"]\n",
    "                center_x_src = bipartites[1].srcdata[\"features\"]\n",
    "\n",
    "            for i in range(len(self.conv)):\n",
    "                neighbor_x_dst = neighbor_x_src[: bipartites[i].num_dst_nodes()]\n",
    "                neighbor_x_src = self.conv[i](\n",
    "                    bipartites[i], (neighbor_x_src, neighbor_x_dst)\n",
    "                )\n",
    "                center_x_dst = center_x_src[: bipartites[i + 1].num_dst_nodes()]\n",
    "                center_x_src = self.conv[i](\n",
    "                    bipartites[i + 1], (center_x_src, center_x_dst)\n",
    "                )\n",
    "\n",
    "            output_bipartite = bipartites[-1]\n",
    "            output_bipartite.srcdata[\"conv_features\"] = neighbor_x_src\n",
    "            output_bipartite.dstdata[\"conv_features\"] = center_x_src\n",
    "\n",
    "        output_bipartite.apply_edges(self.pred_conn)\n",
    "        output_bipartite.edata[\"prob_conn\"] = F.softmax(\n",
    "            output_bipartite.edata[\"pred_conn\"], dim=1\n",
    "        )\n",
    "        output_bipartite.update_all(\n",
    "            self.pred_den_msg, fn.mean(\"pred_den_msg\", \"pred_den\")\n",
    "        )\n",
    "        return output_bipartite\n",
    "\n",
    "    def compute_loss(self, bipartite):\n",
    "        pred_den = bipartite.dstdata[\"pred_den\"]\n",
    "        loss_den = self.loss_den(pred_den, bipartite.dstdata[\"density\"])\n",
    "\n",
    "        labels_conn = bipartite.edata[\"labels_conn\"]\n",
    "        mask_conn = bipartite.edata[\"mask_conn\"]\n",
    "\n",
    "        if self.balance:\n",
    "            labels_conn = bipartite.edata[\"labels_conn\"]\n",
    "            neg_check = torch.logical_and(\n",
    "                bipartite.edata[\"labels_conn\"] == 0, mask_conn\n",
    "            )\n",
    "            num_neg = torch.sum(neg_check).item()\n",
    "            neg_indices = torch.where(neg_check)[0]\n",
    "            pos_check = torch.logical_and(\n",
    "                bipartite.edata[\"labels_conn\"] == 1, mask_conn\n",
    "            )\n",
    "            num_pos = torch.sum(pos_check).item()\n",
    "            pos_indices = torch.where(pos_check)[0]\n",
    "            if num_pos > num_neg:\n",
    "                mask_conn[\n",
    "                    pos_indices[\n",
    "                        np.random.choice(\n",
    "                            num_pos, num_pos - num_neg, replace=False\n",
    "                        )\n",
    "                    ]\n",
    "                ] = 0\n",
    "            elif num_pos < num_neg:\n",
    "                mask_conn[\n",
    "                    neg_indices[\n",
    "                        np.random.choice(\n",
    "                            num_neg, num_neg - num_pos, replace=False\n",
    "                        )\n",
    "                    ]\n",
    "                ] = 0\n",
    "\n",
    "        # In subgraph training, it may happen that all edges are masked in a batch\n",
    "        if mask_conn.sum() > 0:\n",
    "            loss_conn = self.loss_conn(\n",
    "                bipartite.edata[\"pred_conn\"][mask_conn], labels_conn[mask_conn]\n",
    "            )\n",
    "            loss = loss_den + loss_conn\n",
    "            loss_den_val = loss_den.item()\n",
    "            loss_conn_val = loss_conn.item()\n",
    "        else:\n",
    "            loss = loss_den\n",
    "            loss_den_val = loss_den.item()\n",
    "            loss_conn_val = 0\n",
    "\n",
    "        return loss, loss_den_val, loss_conn_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.829310Z",
     "iopub.status.busy": "2025-06-21T10:11:58.829089Z",
     "iopub.status.idle": "2025-06-21T10:11:58.842867Z",
     "shell.execute_reply": "2025-06-21T10:11:58.842307Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.829296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/models/graphconv.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.mlp = nn.Linear(in_feats * 2, out_feats, bias=bias)\n",
    "\n",
    "    def forward(self, bipartite, feat):\n",
    "        if isinstance(feat, tuple):\n",
    "            srcfeat, dstfeat = feat\n",
    "        else:\n",
    "            srcfeat = feat\n",
    "            dstfeat = feat[: bipartite.num_dst_nodes()]\n",
    "        graph = bipartite.local_var()\n",
    "\n",
    "        graph.srcdata[\"h\"] = srcfeat\n",
    "        \n",
    "        ############## OG\n",
    "        graph.update_all(\n",
    "            fn.u_mul_e(\"h\", \"affine\", \"m\"), fn.sum(msg=\"m\", out=\"h\")\n",
    "        )\n",
    "\n",
    "        ############## 1MODIFICA\n",
    "        #si pu testare solo con deepglint in quanto non usa il GAT-layer\n",
    "        #graph.update_all(\n",
    "        #    fn.u_mul_e(\"h\", \"raw_affine\", \"m\"), fn.sum(msg=\"m\", out=\"h\")\n",
    "        #)\n",
    "\n",
    "        ########### 2MODIFICA\n",
    "        #Anche questo si testa senza il modulo GAT, qui pesiamo il messaggio per la densit del nodo sorgente\n",
    "        #graph.srcdata[\"h_density\"] = graph.srcdata[\"h\"] * graph.srcdata[\"density\"].unsqueeze(1)\n",
    "        #graph.update_all(fn.u_mul_e(\"h_density\", \"affine\", \"m\"), fn.sum(msg=\"m\", out=\"h\"))\n",
    "        \n",
    "        gcn_feat = torch.cat([dstfeat, graph.dstdata[\"h\"]], dim=-1)\n",
    "        out = self.mlp(gcn_feat)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0, use_GAT=False, K=1):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        if use_GAT:\n",
    "            self.gcn_layer = GATConv(\n",
    "                in_dim, out_dim, K, allow_zero_in_degree=True\n",
    "            )\n",
    "            self.bias = nn.Parameter(torch.Tensor(K, out_dim))\n",
    "            init.constant_(self.bias, 0)\n",
    "        else:\n",
    "            self.gcn_layer = GraphConvLayer(in_dim, out_dim, bias=True)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_GAT = use_GAT\n",
    "\n",
    "    def forward(self, bipartite, features):\n",
    "        out = self.gcn_layer(bipartite, features)\n",
    "\n",
    "        if self.use_GAT:\n",
    "            out = torch.mean(out + self.bias, dim=1)\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = F.relu(out)\n",
    "        if self.dropout > 0:\n",
    "            out = F.dropout(out, self.dropout, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "###################### 3 MODIFICA\n",
    "#Per usare questa mpdifica rimuovere 1 dal nome della classe ed inserirlo sopra\n",
    "class GraphConv1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0, use_GAT=False, K=1):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        if use_GAT:\n",
    "            self.gcn_layer = GATConv(\n",
    "                in_dim, out_dim, K, allow_zero_in_degree=True\n",
    "            )\n",
    "            self.bias = nn.Parameter(torch.Tensor(K, out_dim))\n",
    "            self.head_weights = nn.Parameter(torch.Tensor(K, out_dim))\n",
    "            init.constant_(self.head_weights, 1)\n",
    "            init.constant_(self.bias, 0)\n",
    "        else:\n",
    "            self.gcn_layer = GraphConvLayer(in_dim, out_dim, bias=True)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_GAT = use_GAT\n",
    "\n",
    "    def forward(self, bipartite, features):\n",
    "        out = self.gcn_layer(bipartite, features)\n",
    "\n",
    "        if self.use_GAT:\n",
    "            #out = torch.mean(out + self.bias, dim=1)\n",
    "            weights = F.softmax(self.head_weights, dim=0)\n",
    "            \n",
    "            out_weighted = torch.zeros_like(out[:, 0, :])\n",
    "            for i in range(out.shape[1]):  # Qui si itera sulle K teste\n",
    "                out_weighted += weights[i] * (out[:, i, :] + self.bias[i])\n",
    "            out = out_weighted\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = F.relu(out)\n",
    "        if self.dropout > 0:\n",
    "            out = F.dropout(out, self.dropout, training=self.training)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.844167Z",
     "iopub.status.busy": "2025-06-21T10:11:58.843951Z",
     "iopub.status.idle": "2025-06-21T10:11:58.856897Z",
     "shell.execute_reply": "2025-06-21T10:11:58.856224Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.844153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/models/focal_loss.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Below code are based on\n",
    "# https://zhuanlan.zhihu.com/p/28527749\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    This criterion is a implemenation of Focal Loss, which is proposed in\n",
    "    Focal Loss for Dense Object Detection.\n",
    "\n",
    "        Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "    The losses are averaged across observations for each minibatch.\n",
    "\n",
    "    Args:\n",
    "        alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "        gamma(float, double) : gamma > 0; reduces the relative loss for well-classied examples (p > .5),\n",
    "                               putting more focus on hard, misclassied examples\n",
    "        size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                            However, if the field size_average is set to False, the losses are\n",
    "                            instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.0)\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P * class_mask).sum(1).view(-1, 1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "\n",
    "        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.858000Z",
     "iopub.status.busy": "2025-06-21T10:11:58.857731Z",
     "iopub.status.idle": "2025-06-21T10:11:58.869180Z",
     "shell.execute_reply": "2025-06-21T10:11:58.868495Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.857955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/models/__init__.py\n",
    "from .graphconv import GraphConv\n",
    "from .lander import LANDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARTELLA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.870117Z",
     "iopub.status.busy": "2025-06-21T10:11:58.869869Z",
     "iopub.status.idle": "2025-06-21T10:11:58.880681Z",
     "shell.execute_reply": "2025-06-21T10:11:58.880128Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.870094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/datasetml/datasetml.py\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import (\n",
    "    build_knns,\n",
    "    build_next_level,\n",
    "    decode,\n",
    "    density_estimation,\n",
    "    fast_knns2spmat,\n",
    "    knns2ordered_nbrs,\n",
    "    l2norm,\n",
    "    row_normalize,\n",
    "    sparse_mx_to_indices_values,\n",
    ")\n",
    "\n",
    "import dgl\n",
    "\n",
    "\n",
    "class LanderDataset(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        labels,\n",
    "        cluster_features=None,\n",
    "        k=10,\n",
    "        levels=1,\n",
    "        faiss_gpu=False,\n",
    "    ):\n",
    "        self.k = k\n",
    "        self.gs = []\n",
    "        self.nbrs = []\n",
    "        self.dists = []\n",
    "        self.levels = levels\n",
    "\n",
    "        # Initialize features and labels\n",
    "        features = l2norm(features.astype(\"float32\"))\n",
    "        global_features = features.copy()\n",
    "        if cluster_features is None:\n",
    "            cluster_features = features\n",
    "        global_num_nodes = features.shape[0]\n",
    "        global_edges = ([], [])\n",
    "        global_peaks = np.array([], dtype=np.longlong)\n",
    "        ids = np.arange(global_num_nodes)\n",
    "\n",
    "        # Recursive graph construction\n",
    "        for lvl in range(self.levels):\n",
    "            if features.shape[0] <= self.k:\n",
    "                self.levels = lvl\n",
    "                break\n",
    "            if faiss_gpu:\n",
    "                knns = build_knns(features, self.k, \"faiss_gpu\")\n",
    "            else:\n",
    "                knns = build_knns(features, self.k, \"faiss\")\n",
    "            dists, nbrs = knns2ordered_nbrs(knns)\n",
    "            self.nbrs.append(nbrs)\n",
    "            self.dists.append(dists)\n",
    "            density = density_estimation(dists, nbrs, labels)\n",
    "\n",
    "            g = self._build_graph(\n",
    "                features, cluster_features, labels, density, knns\n",
    "            )\n",
    "            self.gs.append(g)\n",
    "\n",
    "            if lvl >= self.levels - 1:\n",
    "                break\n",
    "\n",
    "            # Decode peak nodes\n",
    "            (\n",
    "                new_pred_labels,\n",
    "                peaks,\n",
    "                global_edges,\n",
    "                global_pred_labels,\n",
    "                global_peaks,\n",
    "            ) = decode(\n",
    "                g,\n",
    "                0,\n",
    "                \"sim\",\n",
    "                True,\n",
    "                ids,\n",
    "                global_edges,\n",
    "                global_num_nodes,\n",
    "                global_peaks,\n",
    "            )\n",
    "            ids = ids[peaks]\n",
    "            features, labels, cluster_features = build_next_level(\n",
    "                features,\n",
    "                labels,\n",
    "                peaks,\n",
    "                global_features,\n",
    "                global_pred_labels,\n",
    "                global_peaks,\n",
    "            )\n",
    "\n",
    "    def _build_graph(self, features, cluster_features, labels, density, knns):\n",
    "        adj = fast_knns2spmat(knns, self.k)\n",
    "        adj, adj_row_sum = row_normalize(adj)\n",
    "        indices, values, shape = sparse_mx_to_indices_values(adj)\n",
    "\n",
    "        g = dgl.graph((indices[1], indices[0]))\n",
    "        g.ndata[\"features\"] = torch.FloatTensor(features)\n",
    "        g.ndata[\"cluster_features\"] = torch.FloatTensor(cluster_features)\n",
    "        g.ndata[\"labels\"] = torch.LongTensor(labels)\n",
    "        g.ndata[\"density\"] = torch.FloatTensor(density)\n",
    "        g.edata[\"affine\"] = torch.FloatTensor(values)\n",
    "        # A Bipartite from DGL sampler will not store global eid, so we explicitly save it here\n",
    "        g.edata[\"global_eid\"] = g.edges(form=\"eid\")\n",
    "        g.ndata[\"norm\"] = torch.FloatTensor(adj_row_sum)\n",
    "        g.apply_edges(\n",
    "            lambda edges: {\n",
    "                \"raw_affine\": edges.data[\"affine\"] / edges.dst[\"norm\"]\n",
    "            }\n",
    "        )\n",
    "        g.apply_edges(\n",
    "            lambda edges: {\n",
    "                \"labels_conn\": (\n",
    "                    edges.src[\"labels\"] == edges.dst[\"labels\"]\n",
    "                ).long()\n",
    "            }\n",
    "        )\n",
    "        g.apply_edges(\n",
    "            lambda edges: {\n",
    "                \"mask_conn\": (\n",
    "                    edges.src[\"density\"] > edges.dst[\"density\"]\n",
    "                ).bool()\n",
    "            }\n",
    "        )\n",
    "        return g\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index < len(self.gs)\n",
    "        return self.gs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.881524Z",
     "iopub.status.busy": "2025-06-21T10:11:58.881331Z",
     "iopub.status.idle": "2025-06-21T10:11:58.894174Z",
     "shell.execute_reply": "2025-06-21T10:11:58.893638Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.881510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/datasetml/__init__.py\n",
    "from .datasetml import LanderDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.894932Z",
     "iopub.status.busy": "2025-06-21T10:11:58.894731Z",
     "iopub.status.idle": "2025-06-21T10:11:58.910159Z",
     "shell.execute_reply": "2025-06-21T10:11:58.909408Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.894918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/train_subg.py\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv \n",
    "\n",
    "import dgl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/p') \n",
    "sys.path.append('/kaggle/working/p/clustering-benchmark')\n",
    "\n",
    "\n",
    "from datasetml import LanderDataset\n",
    "from models import LANDER\n",
    "\n",
    "###########\n",
    "# ArgParser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument(\"--data_path\", type=str, required=True)\n",
    "parser.add_argument(\"--levels\", type=str, default=\"1\")\n",
    "parser.add_argument(\"--faiss_gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--model_filename\", type=str, default=\"lander.pth\")\n",
    "\n",
    "# KNN\n",
    "parser.add_argument(\"--knn_k\", type=str, default=\"10\")\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\"--hidden\", type=int, default=512)\n",
    "parser.add_argument(\"--num_conv\", type=int, default=1)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.0)\n",
    "parser.add_argument(\"--gat\", action=\"store_true\")\n",
    "parser.add_argument(\"--gat_k\", type=int, default=1)\n",
    "parser.add_argument(\"--balance\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_cluster_feat\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_focal_loss\", action=\"store_true\")\n",
    "\n",
    "# Training\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01)\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
    "\n",
    "#===========================================\n",
    "# NUOVI PARAMETRI PER IL PLOT E SALVATAGGIO LOSS\n",
    "parser.add_argument(\"--save_interval\", type=int, default=50)  # Salva ogni x epoche\n",
    "parser.add_argument(\"--loss_file\", type=str, default=\"losses.csv\")  # File per le losses\n",
    "parser.add_argument(\"--plot_interval\", type=int, default=50)  # Intervallo per il salvataggio del grafico\n",
    "#===========================================\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "###########################\n",
    "# Environment Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "##################\n",
    "# Data Preparation\n",
    "with open(args.data_path, \"rb\") as f:\n",
    "    features, labels = pickle.load(f)\n",
    "\n",
    "k_list = [int(k) for k in args.knn_k.split(\",\")]\n",
    "lvl_list = [int(l) for l in args.levels.split(\",\")]\n",
    "gs = []\n",
    "nbrs = []\n",
    "ks = []\n",
    "for k, l in zip(k_list, lvl_list):\n",
    "    dataset = LanderDataset(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        k=k,\n",
    "        levels=l,\n",
    "        faiss_gpu=args.faiss_gpu,\n",
    "    )\n",
    "    gs += [g for g in dataset.gs]\n",
    "    ks += [k for g in dataset.gs]\n",
    "    nbrs += [nbr for nbr in dataset.nbrs]\n",
    "\n",
    "print(\"Dataset Prepared.\")\n",
    "\n",
    "def set_train_sampler_loader(g, k):\n",
    "    fanouts = [k - 1 for i in range(args.num_conv + 1)]\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(fanouts)\n",
    "    # fix the number of edges\n",
    "    train_dataloader = dgl.dataloading.DataLoader(\n",
    "        g,\n",
    "        torch.arange(g.num_nodes()),\n",
    "        sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "train_loaders = []\n",
    "for gidx, g in enumerate(gs):\n",
    "    train_dataloader = set_train_sampler_loader(gs[gidx], ks[gidx])\n",
    "    train_loaders.append(train_dataloader)\n",
    "\n",
    "##################\n",
    "# Model Definition\n",
    "feature_dim = gs[0].ndata[\"features\"].shape[1]\n",
    "model = LANDER(\n",
    "    feature_dim=feature_dim,\n",
    "    nhid=args.hidden,\n",
    "    num_conv=args.num_conv,\n",
    "    dropout=args.dropout,\n",
    "    use_GAT=args.gat,\n",
    "    K=args.gat_k,\n",
    "    balance=args.balance,\n",
    "    use_cluster_feat=args.use_cluster_feat,\n",
    "    use_focal_loss=args.use_focal_loss,\n",
    ")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "#################\n",
    "# Hyperparameters\n",
    "opt = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "# keep num_batch_per_loader the same for every sub_dataloader\n",
    "num_batch_per_loader = len(train_loaders[0])\n",
    "train_loaders = [iter(train_loader) for train_loader in train_loaders]\n",
    "num_loaders = len(train_loaders)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    opt, T_max=args.epochs * num_batch_per_loader * num_loaders, eta_min=1e-5\n",
    ")\n",
    "\n",
    "print(\"Start Training.\")\n",
    "\n",
    "#==============================================#\n",
    "# CODICE DI TRAINING INVARIATO, \n",
    "# AGGIUNTO SOLO FUNZIONE DI PLOT\n",
    "#==============================================#\n",
    "losses = []\n",
    "\n",
    "def plot_loss(losses, epoch):\n",
    "    epochs = [loss['epoch'] for loss in losses]\n",
    "    loss_vals = [loss['loss'] for loss in losses]\n",
    "    loss_den_vals = [loss['loss_den'] for loss in losses]\n",
    "    loss_conn_vals = [loss['loss_conn'] for loss in losses]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(epochs, loss_vals, label='Loss (D+C)', color='tab:blue', linewidth=2)\n",
    "    plt.plot(epochs, loss_den_vals, label='Density Loss', color='tab:green', linewidth=2)\n",
    "    plt.plot(epochs, loss_conn_vals, label='Connection Loss', color='tab:red', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title(f\"Training Losses\", fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "    plt.savefig(f\"/kaggle/working/loss_plot_epoch_{epoch}.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(args.epochs):\n",
    "    loss_den_val_total = []\n",
    "    loss_conn_val_total = []\n",
    "    loss_val_total = []\n",
    "    for batch in range(num_batch_per_loader):\n",
    "        for loader_id in range(num_loaders):\n",
    "            try:\n",
    "                minibatch = next(train_loaders[loader_id])\n",
    "            except:\n",
    "                train_loaders[loader_id] = iter(\n",
    "                    set_train_sampler_loader(gs[loader_id], ks[loader_id])\n",
    "                )\n",
    "                minibatch = next(train_loaders[loader_id])\n",
    "            input_nodes, sub_g, bipartites = minibatch\n",
    "            sub_g = sub_g.to(device)\n",
    "            bipartites = [b.to(device) for b in bipartites]\n",
    "            # get the feature for the input_nodes\n",
    "            opt.zero_grad()\n",
    "            output_bipartite = model(bipartites)\n",
    "            loss, loss_den_val, loss_conn_val = model.compute_loss(\n",
    "                output_bipartite\n",
    "            )\n",
    "            loss_den_val_total.append(loss_den_val)\n",
    "            loss_conn_val_total.append(loss_conn_val)\n",
    "            loss_val_total.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if (batch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    \"epoch: %d, batch: %d / %d, loader_id : %d / %d, loss: %.6f, loss_den: %.6f, loss_conn: %.6f\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        batch,\n",
    "                        num_batch_per_loader,\n",
    "                        loader_id,\n",
    "                        num_loaders,\n",
    "                        loss.item(),\n",
    "                        loss_den_val,\n",
    "                        loss_conn_val,\n",
    "                    )\n",
    "                )\n",
    "            scheduler.step()\n",
    "\n",
    "    losses.append({\n",
    "        'epoch': epoch,\n",
    "        'loss': np.array(loss_val_total).mean(),\n",
    "        'loss_den': np.array(loss_den_val_total).mean(),\n",
    "        'loss_conn': np.array(loss_conn_val_total).mean()\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        \"epoch: %d, loss: %.6f, loss_den: %.6f, loss_conn: %.6f\"\n",
    "        % (\n",
    "            epoch,\n",
    "            np.array(loss_val_total).mean(),\n",
    "            np.array(loss_den_val_total).mean(),\n",
    "            np.array(loss_conn_val_total).mean(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % args.save_interval == 0:\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")\n",
    "    \n",
    "    # Salvataggio modello finale\n",
    "    torch.save(model.state_dict(), args.model_filename)\n",
    "\n",
    "    # Salvataggio del grafico\n",
    "    if (epoch + 1) % args.plot_interval == 0:\n",
    "        plot_loss(losses, epoch)\n",
    "\n",
    "with open(args.loss_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['epoch', 'loss', 'loss_den', 'loss_conn'])\n",
    "    writer.writeheader()\n",
    "    for loss in losses:\n",
    "        writer.writerow(loss)\n",
    "\n",
    "\"\"\"\n",
    "###############\n",
    "# Training Loop\n",
    "for epoch in range(args.epochs):\n",
    "    loss_den_val_total = []\n",
    "    loss_conn_val_total = []\n",
    "    loss_val_total = []\n",
    "    for batch in range(num_batch_per_loader):\n",
    "        for loader_id in range(num_loaders):\n",
    "            try:\n",
    "                minibatch = next(train_loaders[loader_id])\n",
    "            except:\n",
    "                train_loaders[loader_id] = iter(\n",
    "                    set_train_sampler_loader(gs[loader_id], ks[loader_id])\n",
    "                )\n",
    "                minibatch = next(train_loaders[loader_id])\n",
    "            input_nodes, sub_g, bipartites = minibatch\n",
    "            sub_g = sub_g.to(device)\n",
    "            bipartites = [b.to(device) for b in bipartites]\n",
    "            # get the feature for the input_nodes\n",
    "            opt.zero_grad()\n",
    "            output_bipartite = model(bipartites)\n",
    "            loss, loss_den_val, loss_conn_val = model.compute_loss(\n",
    "                output_bipartite\n",
    "            )\n",
    "            loss_den_val_total.append(loss_den_val)\n",
    "            loss_conn_val_total.append(loss_conn_val)\n",
    "            loss_val_total.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if (batch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    \"epoch: %d, batch: %d / %d, loader_id : %d / %d, loss: %.6f, loss_den: %.6f, loss_conn: %.6f\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        batch,\n",
    "                        num_batch_per_loader,\n",
    "                        loader_id,\n",
    "                        num_loaders,\n",
    "                        loss.item(),\n",
    "                        loss_den_val,\n",
    "                        loss_conn_val,\n",
    "                    )\n",
    "                )\n",
    "            scheduler.step()\n",
    "    print(\n",
    "        \"epoch: %d, loss: %.6f, loss_den: %.6f, loss_conn: %.6f\"\n",
    "        % (\n",
    "            epoch,\n",
    "            np.array(loss_val_total).mean(),\n",
    "            np.array(loss_den_val_total).mean(),\n",
    "            np.array(loss_conn_val_total).mean(),\n",
    "        )\n",
    "    )\n",
    "    torch.save(model.state_dict(), args.model_filename)\n",
    "\n",
    "torch.save(model.state_dict(), args.model_filename)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.911388Z",
     "iopub.status.busy": "2025-06-21T10:11:58.910920Z",
     "iopub.status.idle": "2025-06-21T10:11:58.922372Z",
     "shell.execute_reply": "2025-06-21T10:11:58.921663Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.911366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#========== TRAIN INAT2018\n",
    "#%run ./p/train_subg.py --data_path /kaggle/input/dataset-ml/inat2018_train_dedup_inter_intra.pkl --knn_k 10,5,3 --levels 2,3,4 --faiss_gpu --hidden 512 --epochs 250 --lr 0.01 --batch_size 4096 --num_conv 1 --gat --gat_k 2 --balance\n",
    "\n",
    "#========== TRAIN INAT PIC\n",
    "#%run ./p/train_subg.py --data_path /kaggle/input/dataset-ml/inat2018_train_dedup_inter_intra_1_in_6_per_class.pkl --knn_k 10,5,3 --levels 2,3,4 --faiss_gpu --hidden 512 --epochs 250 --lr 0.01 --batch_size 4096 --num_conv 1 --gat --balance\n",
    "\n",
    "#========== TRAIN DEEPLIGHT\n",
    "#%run ./p/train_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_train_1_in_10_recreated.pkl --knn_k 10,5,3 --levels 2,3,4 --faiss_gpu --hidden 512 --epochs 250 --lr 0.01 --batch_size 4096 --num_conv 1 --balance --use_cluster_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.944359Z",
     "iopub.status.busy": "2025-06-21T10:11:58.944163Z",
     "iopub.status.idle": "2025-06-21T10:11:58.954844Z",
     "shell.execute_reply": "2025-06-21T10:11:58.954302Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.944337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./p/test_subg.py\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/p')  # root del progetto\n",
    "sys.path.append('/kaggle/working/p/clustering-benchmark')\n",
    "\n",
    "import dgl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from datasetml import LanderDataset\n",
    "from models import LANDER\n",
    "from utils import build_next_level_mod7, decode, evaluation, stop_iterating\n",
    "\n",
    "###########\n",
    "# ArgParser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument(\"--data_path\", type=str, required=True)\n",
    "parser.add_argument(\"--model_filename\", type=str, default=\"lander.pth\")\n",
    "parser.add_argument(\"--faiss_gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "\n",
    "# HyperParam\n",
    "parser.add_argument(\"--knn_k\", type=int, default=10)\n",
    "parser.add_argument(\"--levels\", type=int, default=1)\n",
    "parser.add_argument(\"--tau\", type=float, default=0.5)\n",
    "parser.add_argument(\"--threshold\", type=str, default=\"prob\")\n",
    "parser.add_argument(\"--metrics\", type=str, default=\"pairwise,bcubed,nmi\")\n",
    "parser.add_argument(\"--early_stop\", action=\"store_true\")\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\"--hidden\", type=int, default=512)\n",
    "parser.add_argument(\"--num_conv\", type=int, default=4)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.0)\n",
    "parser.add_argument(\"--gat\", action=\"store_true\")\n",
    "parser.add_argument(\"--gat_k\", type=int, default=1)\n",
    "parser.add_argument(\"--balance\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_cluster_feat\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_focal_loss\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_gt\", action=\"store_true\")\n",
    "\n",
    "# Subgraph\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4096)\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "###########################\n",
    "# Environment Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "##################\n",
    "# Data Preparation\n",
    "with open(args.data_path, \"rb\") as f:\n",
    "    features, labels = pickle.load(f)\n",
    "global_features = features.copy()\n",
    "dataset = LanderDataset(\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    k=args.knn_k,\n",
    "    levels=1,\n",
    "    faiss_gpu=args.faiss_gpu,\n",
    ")\n",
    "g = dataset.gs[0]\n",
    "g.ndata[\"pred_den\"] = torch.zeros((g.num_nodes()))\n",
    "g.edata[\"prob_conn\"] = torch.zeros((g.num_edges(), 2))\n",
    "global_labels = labels.copy()\n",
    "ids = np.arange(g.num_nodes())\n",
    "global_edges = ([], [])\n",
    "global_peaks = np.array([], dtype=np.longlong)\n",
    "global_edges_len = len(global_edges[0])\n",
    "global_num_nodes = g.num_nodes()\n",
    "\n",
    "fanouts = [args.knn_k - 1 for i in range(args.num_conv + 1)]\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler(fanouts)\n",
    "# fix the number of edges\n",
    "test_loader = dgl.dataloading.DataLoader(\n",
    "    g,\n",
    "    torch.arange(g.num_nodes()),\n",
    "    sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "##################\n",
    "# Model Definition\n",
    "if not args.use_gt:\n",
    "    feature_dim = g.ndata[\"features\"].shape[1]\n",
    "    model = LANDER(\n",
    "        feature_dim=feature_dim,\n",
    "        nhid=args.hidden,\n",
    "        num_conv=args.num_conv,\n",
    "        dropout=args.dropout,\n",
    "        use_GAT=args.gat,\n",
    "        K=args.gat_k,\n",
    "        balance=args.balance,\n",
    "        use_cluster_feat=args.use_cluster_feat,\n",
    "        use_focal_loss=args.use_focal_loss,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(args.model_filename, weights_only=True))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "# number of edges added is the indicator for early stopping\n",
    "num_edges_add_last_level = np.Inf\n",
    "##################################\n",
    "# Predict connectivity and density\n",
    "for level in range(args.levels):\n",
    "    if not args.use_gt:\n",
    "        total_batches = len(test_loader)\n",
    "        for batch, minibatch in enumerate(test_loader):\n",
    "            input_nodes, sub_g, bipartites = minibatch\n",
    "            sub_g = sub_g.to(device)\n",
    "            bipartites = [b.to(device) for b in bipartites]\n",
    "            with torch.no_grad():\n",
    "                output_bipartite = model(bipartites)\n",
    "            global_nid = output_bipartite.dstdata[dgl.NID]\n",
    "            global_eid = output_bipartite.edata[\"global_eid\"]\n",
    "            g.ndata[\"pred_den\"][global_nid] = output_bipartite.dstdata[\n",
    "                \"pred_den\"\n",
    "            ].to(\"cpu\")\n",
    "            g.edata[\"prob_conn\"][global_eid] = output_bipartite.edata[\n",
    "                \"prob_conn\"\n",
    "            ].to(\"cpu\")\n",
    "            torch.cuda.empty_cache()\n",
    "            if (batch + 1) % 10 == 0:\n",
    "                print(\"Batch %d / %d for inference\" % (batch, total_batches))\n",
    "\n",
    "    (\n",
    "        new_pred_labels,\n",
    "        peaks,\n",
    "        global_edges,\n",
    "        global_pred_labels,\n",
    "        global_peaks,\n",
    "    ) = decode(\n",
    "        g,\n",
    "        args.tau,\n",
    "        args.threshold,\n",
    "        args.use_gt,\n",
    "        ids,\n",
    "        global_edges,\n",
    "        global_num_nodes,\n",
    "        global_peaks,\n",
    "    )\n",
    "    ids = ids[peaks]\n",
    "    new_global_edges_len = len(global_edges[0])\n",
    "    num_edges_add_this_level = new_global_edges_len - global_edges_len\n",
    "    if stop_iterating(\n",
    "        level,\n",
    "        args.levels,\n",
    "        args.early_stop,\n",
    "        num_edges_add_this_level,\n",
    "        num_edges_add_last_level,\n",
    "        args.knn_k,\n",
    "    ):\n",
    "        break\n",
    "    global_edges_len = new_global_edges_len\n",
    "    num_edges_add_last_level = num_edges_add_this_level\n",
    "\n",
    "    # build new dataset\n",
    "    features, labels, cluster_features = build_next_level_mod7(\n",
    "        features,\n",
    "        labels,\n",
    "        peaks,\n",
    "        global_features,\n",
    "        global_pred_labels,\n",
    "        global_peaks,\n",
    "    )\n",
    "    # After the first level, the number of nodes reduce a lot. Using cpu faiss is faster.\n",
    "    dataset = LanderDataset(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        k=args.knn_k,\n",
    "        levels=1,\n",
    "        faiss_gpu=False,\n",
    "        cluster_features=cluster_features,\n",
    "    )\n",
    "    g = dataset.gs[0]\n",
    "    g.ndata[\"pred_den\"] = torch.zeros((g.num_nodes()))\n",
    "    g.edata[\"prob_conn\"] = torch.zeros((g.num_edges(), 2))\n",
    "    test_loader = dgl.dataloading.DataLoader(\n",
    "        g,\n",
    "        torch.arange(g.num_nodes()),\n",
    "        sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "evaluation(global_pred_labels, global_labels, args.metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:58.955759Z",
     "iopub.status.busy": "2025-06-21T10:11:58.955533Z",
     "iopub.status.idle": "2025-06-21T10:11:58.966359Z",
     "shell.execute_reply": "2025-06-21T10:11:58.965758Z",
     "shell.execute_reply.started": "2025-06-21T10:11:58.955739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TEST INAT FULL\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlreplica/og/inat2018_1/lander_inat.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop\n",
    "\n",
    "#TEST DEEPGLINT\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlreplica/og/deepglint/lander_deeplight.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "# TEST HANNAH\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlreplica/og/deepglint/lander_deeplight.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#IMDB\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlreplica/og/deepglint/lander_deeplight.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#inat_2018\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/working/model_epoch_3.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.011502Z",
     "iopub.status.busy": "2025-06-21T10:11:59.011321Z",
     "iopub.status.idle": "2025-06-21T10:11:59.019416Z",
     "shell.execute_reply": "2025-06-21T10:11:59.018877Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.011486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "######### MODIFICA 1\n",
    "# TEST SI PU FARE SOLO SU TEST DATa DI DEEPGLINT\n",
    "\n",
    "#TEST DEEPGLINT SEENDATA\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlmodifica1/1mod/lander_inat_2.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#TEST HANNAH UNSEEN\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlmodifica1/1mod/lander_inat_2.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#TEST IMBD unseeN\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlmodifica1/1mod/lander_inat_2.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.020166Z",
     "iopub.status.busy": "2025-06-21T10:11:59.020004Z",
     "iopub.status.idle": "2025-06-21T10:11:59.032467Z",
     "shell.execute_reply": "2025-06-21T10:11:59.031689Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.020153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#################### MODIFICA 2\n",
    "# TEST SI PU FARE SOLO SU TEST DATa DI DEEPGLINT\n",
    "\n",
    "#TEST DEEPGLINT SEENDATA\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlmod2/2mod/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#TEST HANNAH UNSEEN\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlmod2/2mod/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#TEST IMBD unseeN\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlmod2/2mod/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.033407Z",
     "iopub.status.busy": "2025-06-21T10:11:59.033186Z",
     "iopub.status.idle": "2025-06-21T10:11:59.046238Z",
     "shell.execute_reply": "2025-06-21T10:11:59.045612Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.033386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "############## MODIFICA 3\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mod3gat/3mod/lander_inat_piccolo.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --gat_k 4 --batch_size 4096 --early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.047090Z",
     "iopub.status.busy": "2025-06-21T10:11:59.046855Z",
     "iopub.status.idle": "2025-06-21T10:11:59.056429Z",
     "shell.execute_reply": "2025-06-21T10:11:59.055898Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.047071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###################### MODIFICA 4\n",
    "# TEST INAT FULL\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod4/4mod/inat_full/lander_inat.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop\n",
    "\n",
    "#TEST DEEPGLINT\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlmod4/4mod/deepglint/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "# TEST HANNAH\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlmod4/4mod/deepglint/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#IMDB\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlmod4/4mod/deepglint/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#inat_2018\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod4/4mod/inat_piccolo/lander_inat_2.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.057197Z",
     "iopub.status.busy": "2025-06-21T10:11:59.056992Z",
     "iopub.status.idle": "2025-06-21T10:11:59.068523Z",
     "shell.execute_reply": "2025-06-21T10:11:59.067879Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.057184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "############################# MODIFICA 5\n",
    "# TEST INAT FULL\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod6/5mod/inat_full/lander_inat.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop\n",
    "\n",
    "#TEST DEEPGLINT\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlmod6/5mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "# TEST HANNAH\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlmod6/5mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#IMDB\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlmod6/5mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#inat_2018\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod6/5mod/inat_piccolo/lander_inat_piccolo.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.069340Z",
     "iopub.status.busy": "2025-06-21T10:11:59.069157Z",
     "iopub.status.idle": "2025-06-21T10:11:59.081196Z",
     "shell.execute_reply": "2025-06-21T10:11:59.080444Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.069321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "############################# MODIFICA 6\n",
    "# TEST INAT FULL\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod6-1/6mod/inat_full/lander_inat_pic.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop\n",
    "\n",
    "#TEST DEEPGLINT\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features_sampled_as_deepglint_1_in_10.pkl --model_filename /kaggle/input/mlmod6-1/6mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "# TEST HANNAH\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_hannah.pkl --model_filename /kaggle/input/mlmod6-1/6mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#IMDB\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/subcenter_arcface_deepglint_imdb_features.pkl --model_filename /kaggle/input/mlmod6-1/6mod/deepglit/lander_deepglint.pth --knn_k 10 --tau 0.8 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --batch_size 4096 --early_stop --use_cluster_feat\n",
    "\n",
    "#inat_2018\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod6-1/6mod/inat_piccolo/lander_inat_2.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:11:59.082289Z",
     "iopub.status.busy": "2025-06-21T10:11:59.082120Z",
     "iopub.status.idle": "2025-06-21T10:13:32.857187Z",
     "shell.execute_reply": "2025-06-21T10:13:32.856464Z",
     "shell.execute_reply.started": "2025-06-21T10:11:59.082273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "############################# MODIFICA 7\n",
    "#inat_2018\n",
    "#%run ./p/test_subg.py --data_path /kaggle/input/dataset-ml/inat2018_test.pkl --model_filename /kaggle/input/mlmod7/7mod/s10000/lander.pth --knn_k 10 --tau 0.1 --level 10 --threshold prob --faiss_gpu --hidden 512 --num_conv 1 --gat --batch_size 4096 --early_stop"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7461303,
     "sourceId": 11872656,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7683898,
     "sourceId": 12198235,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7699698,
     "sourceId": 12221342,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7699862,
     "sourceId": 12221601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7701694,
     "sourceId": 12224390,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7701936,
     "sourceId": 12224721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7704619,
     "sourceId": 12228544,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7707096,
     "sourceId": 12232051,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709117,
     "sourceId": 12235194,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
